# NdLinear Benchmarking in Reinforcement Learning

## ðŸ“Œ Overview
This project benchmarks [NdLinear](https://github.com/ensemble-core/NdLinear), a multi-space representation layer, against standard `nn.Linear` in reinforcement learning environments using policy gradient methods.

We test performance in two environments:

- ðŸŽ¯ **CartPole-v1**
- ðŸš€ **LunarLander-v2**

---

## ðŸš€ Objective
Evaluate whether `NdLinear` improves learning efficiency, reward stability, and final performance in reinforcement learning tasks compared to standard linear layers.

---

## ðŸ§  Methodology

- **Environments**: CartPole-v1, LunarLander-v2 (OpenAI Gym)
- **Model Types**:
  - `NdLinearPolicy`: Uses NdLinear layers
  - `LinearPolicy`: Uses PyTorch nn.Linear
- **Algorithm**: REINFORCE (policy gradient)
- **Training Length**:
  - CartPole: 1000 episodes
  - LunarLander: 2000 episodes
- **Metrics**: Total reward per episode

---

## ðŸ—ï¸ Architecture

| Model          | Structure                                           |
|----------------|-----------------------------------------------------|
| NdLinearPolicy | NdLinear(Obs -> Hidden) â†’ ReLU â†’ NdLinear â†’ Softmax |
| LinearPolicy   | nn.Linear(Obs -> Hidden) â†’ ReLU â†’ nn.Linear â†’ Softmax |

---

## ðŸ“ˆ Results

### ðŸŸ¢ CartPole-v1

- NdLinear converged faster and reached the 500 reward ceiling more consistently.
- LinearPolicy was less stable and occasionally regressed.

![CartPole Reward Comparison](cartpole/Figure_1.png)

---

### ðŸ”µ LunarLander-v2

- NdLinear shows significant learning progress after 1000 episodes.
- LinearPolicy struggled with negative rewards and did not learn a stable policy.

![LunarLander Reward Comparison](lunarlander/output.png)

---

## ðŸ“¦ Project Structure

```
NdLinear/
â”œâ”€â”€ cartpole/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ train_linear.py
â”‚   â”œâ”€â”€ plot_comparison.py
â”‚   â”œâ”€â”€ rewards_log.json
â”‚   â”œâ”€â”€ rewards_log_linear.json
â”‚   â””â”€â”€ Figure_1.png
â”œâ”€â”€ lunarlander/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ train_linear.py
â”‚   â”œâ”€â”€ plot_comparison.py
â”‚   â”œâ”€â”€ rewards_log.json
â”‚   â”œâ”€â”€ rewards_log_linear.json
â”‚   â””â”€â”€ Figure_2.png
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ndlinear_policy.py
â”‚   â”œâ”€â”€ linear_policy.py
â”‚   â””â”€â”€ local_ndlinear.py
â””â”€â”€ README.md
```

---

## ðŸ§ª How to Run

### Train NdLinear
```bash
python cartpole/train.py
python lunarlander/train.py
```

### Train nn.Linear baseline
```bash
python cartpole/train_linear.py
python lunarlander/train_linear.py
```

### Plot reward comparisons
```bash
python cartpole/plot_comparison.py
python lunarlander/plot_comparison.py
```

---

## ðŸ‘¤ Author
**Aryan Patodiya**  
M.S. Computer Science @ CSU Fresno  
[GitHub Profile](https://github.com/aryanpatodiya)

---

## âœ… Submission Checklist

- [x] Starred the NdLinear GitHub repo
- [x] Benchmarked on two environments
- [x] Compared NdLinear vs nn.Linear performance
- [x] Public repo with logs, plots, and README
